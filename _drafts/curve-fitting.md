---
layout: post
title:  "Curve Fitting"
categories: mathematics
---

Suppose we have some data points from a measurement or observation and want to fit some function $$ y = \phi(x) $$ to analyse or predict behaviour. How do we determine the "best" predictor function $$ \phi $$?

![Data Points]({{ site.url }}/images/curve-fitting-linear.svg){: .center-image }
![Data Points]({{ site.url }}/images/curve-fitting-quadratic.svg){: .center-image }

The predictor function $$ \phi $$ should satisfy $$ y_k \approx \phi(x_k) $$ for each datapoint $$ (x_k, y_k) $$ in the dataset. To make this precise, define the residuals

$$ r_k = \phi(x_k) - y_k $$

for all $$ k $$. Graphically, this represents the vertical difference between the data points and the prediction.

![Residuals]({{ site.url }}/images/curve-fitting-residual.svg){: .center-image}

Given $$ n $$ datapoints, define the vectors $$ \vec{x} = (x_1, x_2, \ldots, x_n)^\top $$, $$ \vec{y} = (y_1, y_2, \ldots, y_n)^\top $$ and the residual vector $$ \vec{r} = (r_1, r_2, \ldots, r_n)^\top $$. If $$ \vec{r} = \vec{0} $$, then $$ \phi $$ is an exact fit which interpolates the data. This is often not useful, especially when there are a large number of data points. Instead, a simple function $$ \phi $$ is desired which produces a small norm $$ \| \vec{r} \| $$.

## Least Squares

Consider parametric functions of the form

$$ \phi(x; c_1, c_2, \ldots, c_m) = c_1 \phi_1(x) + c_2 \phi_2(x) + \cdots + c_m \phi_m(x) $$

where $$ \phi_1, \phi_2, \ldots, \phi_m $$ are known, that is, they do not depend on any parameters. These $$ \phi_k $$ are called **basis functions**. Using the relationship $$ r_k = \phi(x_k) - y_k $$ and the above expansion of $$ \phi $$, we can write

$$ 
\begin{pmatrix}r_1\\r_2\\\vdots\\r_n\end{pmatrix} = \begin{bmatrix}
\phi_1(x_1) & \phi_2(x_1) & \cdots & \phi_m(x_1)\\
\phi_1(x_2) & \phi_2(x_2) & \cdots & \phi_m(x_2)\\
\vdots & \vdots &  & \vdots \\
\phi_1(x_n) & \phi_2(x_2) & \cdots & \phi_m(x_n)\\
\end{bmatrix}
\begin{pmatrix}c_1\\c_2\\\vdots\\c_m\end{pmatrix}
-
\begin{pmatrix}y_1\\y_2\\\vdots\\y_n\end{pmatrix}.
$$

In vector form,

$$ \vec{r} = A\vec{c} - \vec{y} $$

for the appropriately defined matrix $$ A \in \mathbf{R}^{n \times m} $$ and the coefficient vector $$ \vec{c} \in \mathbf{R}^m $$. We now seek to minimise the norm $$ \| \vec{r} \| $$ by varying the parameters contained in the coefficient vector $$ \vec{c} $$.

In other words, we want to find a specific set of parameters $$ \vec{c}^* $$ and corresponding residual vector $$ \vec{r}^* $$ such that

$$ \| \vec{r}^* \| = \| A\vec{c}^* - \vec{y} \| \le \| A\vec{c} - \vec{y} \| = \| \vec{r} \| $$

for all vectors $$ \vec{c} $$ and residuals $$ \vec{r} $$. To do this, we can invoke a theorem from linear algebra.

<blockquote class="theorem" markdown="1">
**Theorem.** Suppose $$ W $$ is a subspace of a finite inner product space $$ V $$ and $$ \vec{v} \in V $$. Then

$$ \| \vec{v} - \operatorname{proj}_W(\vec{v}) \| \leqslant \| \vec{v} - \vec{w} \| $$

for all $$ \vec{w} \in W $$. Moreover, equality occurs if and only if $$ \vec{w} = \operatorname{proj}_W(\vec{v}) $$.

> _Proof._ Suppose $$ \vec{w} \in W $$, then
>
> $$\begin{align*}
\| \vec{v} - \operatorname{proj}_W(\vec{v}) \|^2
&\leqslant \| \vec{v} - \operatorname{proj}_W(\vec{v}) \|^2 + \| \operatorname{proj}_W(\vec{v}) - \vec{w} \|^2\\
&= \| \vec{v} - \vec{w} \|^2,
\end{align*}$$
>
> by Pythagoras' Theorem. Taking positive square roots yields the inequality. If equality holds, then $$ \| \vec{v} - \operatorname{proj}_W(\vec{v}) \|^2 = \| \vec{v} - \vec{w} \|^2 $$ so $$ \| \operatorname{proj}_W(\vec{v}) - \vec{w} \|^2 = 0 $$ which holds if and only if $$ \operatorname{proj}_W(\vec{v}) = \vec{w} $$.
>
> ![Projection]({{ site.url }}/images/curve-fitting-projection.svg){: .center-image }

</blockquote>

Let $$ W = \operatorname{col}(A) $$ be the subspace generated by the columns of the matrix $$ A $$. The theorem states that $$ \| A\vec{c} - \vec{y} \| $$ is minimised when $$ A \vec{c} $$ is the projection of $$ \vec{y} $$ onto the subspace $$ W $$,

$$ A \vec{c}^* = \operatorname{proj}_{W}(\vec{y}). $$

Note that

$$ A \vec{c}^* - \vec{y} = \operatorname{proj}_W(\vec{y}) - \vec{y} \in \operatorname{col}(A)^\perp = \operatorname{ker}(A^\top). $$

Thus, $$ A^\top(A\vec{c}^* - \vec{y}) = \vec{0} $$ and we have derived the **normal equations**

$$ A^\top A \vec{c}^* = A^\top \vec{y}. $$

Note that there is always a solution to this equation and it may not be unique. Therefore, given a parametric function $$ \phi $$ we can find the optimal set of parameters that minimise the sum of squares of residuals by solving the normal equations. It is best to compute $$ \vec{c} $$ using a QR-factorisation rather than forming $$ A^\top A $$ because it is numerically unstable as $$ \kappa(A^\top A) = \kappa(A)^2 $$.

The difficulty lies in finding an appropriate form of function

| Relationship | Equation | Linearised Equation ($$ \phi(x) $$) |
| line | $$mx + b $$ | $$ y = mx + b $$ |
| polynomial | $$ax^3 + bx^2 + cx + d $$ | $$y = ax^3 + bx^2 + cx + d $$ |
| expontential | $$Ae^{kx} $$ | $$ \log(y) = \log(A) + kx $$ |
| trigonometric | $$ A\cos(x) + B\sin(x) $$ | $$ y = A\cos(x) + B\sin(x) $$ |
| trigonometric | $$\cos(\omega x + \varphi) $$ | $$ \cos^{-1}(y) = \omega x + \varphi $$ |
| hyperbolic | $$\dfrac{1}{b + cx}$$ | $$\dfrac{1}{y} = b + cx $$ |

$$ \| \vec{r} \| = \sqrt{ {r_1}^2 + {r_2}^2 + \cdots + {r_n}^2 } . $$

pitfall: Anscombe's quartet
Gaussâ€“Markov theorem

## Statistics

We have for all $$ k $$,

$$ \phi(x_k) = y_k + r_k $$

where $$ E[r_k] = 0 $$.

- Statistics Stuff
- Machine Learning: Gradient Descent